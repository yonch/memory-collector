name: test-ebpf-collector
on: 
  workflow_dispatch:  # Manual trigger for testing
    inputs:
      instance-type:
        description: 'EC2 instance type to use'
        required: false
        default: 'c7i.metal-24xl'
        type: string
  push:
    branches:
      - main
    paths:
      - crates/**
      - Cargo.toml
      - .github/workflows/test-ebpf-collector.yaml

permissions:
  id-token: write # Required for requesting the JWT
  actions: write # To cancel the workflow if getting the AWS instance fails

jobs:
  build-collector:
    name: Build eBPF collector
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y clang libelf-dev unzip

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cargo cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Test cgroup inode assumptions
        run: |
          # Build the test, but don't run it yet
          cargo test --package bpf --test cgroup_inode_test --no-run --verbose
          
          echo "Testing that the cgroup ID from BPF matches the inode number in the filesystem"
          echo "This is critical for container identification in the collector"
          
          # Find the test binary
          TEST_BIN=$(find target/debug -name "cgroup_inode_test-*" -type f -executable | head -1)
          
          if [ -z "$TEST_BIN" ]; then
            echo "Could not find test binary. Checking for alternative locations..."
            TEST_BIN=$(find . -name "cgroup_inode_test-*" -type f -executable | head -1)
          fi
          
          if [ -z "$TEST_BIN" ]; then
            echo "Error: Could not find test binary anywhere."
            exit 1
          fi
          
          echo "Found test binary at: $TEST_BIN"
          # Run the test as root
          sudo $TEST_BIN

      - name: Build collector
        run: |
          cargo build
          mkdir -p artifacts
          cp target/debug/collector artifacts/

      - name: Upload collector binary
        uses: actions/upload-artifact@v4
        with:
          name: collector-binary
          path: artifacts/collector

  test-multi-kernel:
    name: Test on Multiple Kernels
    needs: [build-collector, setup-runner, prepare-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - kernel: '5.10'
            should_succeed: false
            description: 'Kernel 5.10 (should fail - does not have bpf timer support)'
          # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
          - kernel: '5.15'
            should_succeed: true
            description: 'Kernel 5.15 (should succeed - legacy timer mode - relative time supported)'
          # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
          - kernel: '6.1'
            should_succeed: true
            description: 'Kernel 6.1 (should succeed - legacy timer mode - relative time supported)'
          # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
          - kernel: '6.6'
            should_succeed: true
            description: 'Kernel 6.6 (should succeed - intermediate timer mode - absolute time supported)'
          # renovate: datasource=docker depName=quay.io/lvh-images/complexity-test
          - kernel: '6.12'
            should_succeed: true
            description: 'Kernel 6.12 (should succeed - modern timer mode - CPU pinning + absolute time supported)'
    timeout-minutes: 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: collector-binary
          path: ./artifacts

      - name: Provision LVH VM and Test
        # uses: cilium/little-vm-helper@e87948476ca97050b1f149ab2aec379d0de19b84 # v0.0.23
        uses: yonch/little-vm-helper@main
        with:
          test-name: collector-kernel-${{ matrix.kernel }}
          image: 'complexity-test'
          image-version: ${{ matrix.kernel }}
          host-mount: ./
          images-folder-parent: "/tmp"
          cpu: 2
          mem: 2G
          cpu-kind: 'host,pmu=on'
          lvh-version: "v0.0.23"
          install-dependencies: 'true'
          verbose: 'true'
          cmd: |
            # Wait for networking to be ready
            for i in {1..5}; do curl "https://golang.org" > /dev/null 2>&1 && break || sleep 5; echo "Waiting for systemd-resolved to be ready..."; done
            
            git config --global --add safe.directory /host
            uname -a
            echo "Testing memory collector on kernel: ${{ matrix.description }}"
            
            # Check if perf events are available
            echo "Checking perf event capabilities..."
            ls -la /sys/bus/event_source/devices/ || echo "No perf event devices found"
            ls -la /proc/sys/kernel/perf_event_* || echo "No perf event sysctls found"
            cat /proc/cpuinfo | grep -i "pmu\|perf" || echo "No PMU features in cpuinfo"
            echo "perf_event_paranoid: $(cat /proc/sys/kernel/perf_event_paranoid)"
            echo 0 > /proc/sys/kernel/perf_event_paranoid

            echo "available_clocksource:"
            cat /sys/devices/system/clocksource/clocksource0/available_clocksource || echo "No available clocksource found"
            echo "current_clocksource:"
            cat /sys/devices/system/clocksource/clocksource0/current_clocksource || echo "No current clocksource found"

            echo "timer_list:"
            cat /proc/timer_list || echo "No timer list found"
            echo "dmesg timer:"
            dmesg | grep -i timer || echo "No timer found in dmesg"

            echo mounts:
            mount

            echo "Debug: Dollar-question-mark value: $?"
            COLLECTOR_EXIT_CODE=0
            echo "Debug: Assigned value: $COLLECTOR_EXIT_CODE"
            echo "Debug: Assigned value (escaped dollar sign): \$COLLECTOR_EXIT_CODE"

            uname -a
            cd /host
            
            # Make collector executable
            chmod +x ./artifacts/collector
            
            echo "=== Testing memory collector on ${{ matrix.description }} ==="
            echo "Kernel version: $(uname -a)"
            echo "Expected result: ${{ matrix.should_succeed && 'SUCCESS' || 'FAILURE with kernel version error' }}"
            echo
            
            # Set up test environment
            mkdir -p /tmp/test-output
            
            # Run the collector and capture both stdout and stderr
            echo "Running collector for 5 seconds..."
            set +e  # Don't exit on error
            ./artifacts/collector -d 5 --storage-type local --prefix "/tmp/test-output/metrics-" > /tmp/collector-output.log 2>&1
            COLLECTOR_EXIT_CODE=$?
            set -e
            
            echo "=== Collector Output ==="
            cat /tmp/collector-output.log
            echo "========================"
            echo "Collector exit code: $COLLECTOR_EXIT_CODE"
            echo
            
            # Handle empty exit code (fallback for safety)
            if [ -z "$COLLECTOR_EXIT_CODE" ]; then
              echo "Error: Exit code is empty (unexpected)"
              exit 1
            fi
            
            # Check if the behavior matches expectations
            if [ "${{ matrix.should_succeed }}" = "true" ]; then
              # For kernels 6.7+, expect success
              if [ "$COLLECTOR_EXIT_CODE" -eq "0" ]; then
                echo "✅ SUCCESS: Collector ran successfully on supported kernel"
                
                # Verify parquet files were created
                if ls /tmp/test-output/metrics-*.parquet >/dev/null 2>&1; then
                  echo "✅ SUCCESS: Parquet files were created"
                  echo "Created files:"
                  ls -la /tmp/test-output/metrics-*.parquet
                else
                  echo "❌ UNEXPECTED: No parquet files found despite successful run"
                  exit 1
                fi
              else
                echo "❌ UNEXPECTED: Collector failed on supported kernel (exit code: $COLLECTOR_EXIT_CODE)"
                exit 1
              fi
            else
              # For kernels < 6.7, expect failure with kernel error message
              if [ "$COLLECTOR_EXIT_CODE" -ne "0" ]; then
                echo "✅ EXPECTED: Collector failed on unsupported kernel (exit code: $COLLECTOR_EXIT_CODE)"
                
                # Check if the error message mentions kernel 6.7 requirement
                if grep -i "kernel 6\.7" /tmp/collector-output.log; then
                  echo "✅ SUCCESS: Error message correctly mentions kernel 6.7 requirement"
                else
                  echo "❌ IMPROVEMENT NEEDED: Error message should mention kernel 6.7 requirement"
                  echo "Current error output:"
                  cat /tmp/collector-output.log
                  # Don't fail the test, just note the improvement needed
                fi
                
                # Verify no parquet files were created
                if ! ls /tmp/test-output/metrics-*.parquet >/dev/null 2>&1; then
                  echo "✅ SUCCESS: No parquet files created on unsupported kernel"
                else
                  echo "❌ UNEXPECTED: Parquet files were created despite failure"
                  exit 1
                fi
              else
                echo "❌ UNEXPECTED: Collector succeeded on unsupported kernel"
                echo "This suggests the kernel version detection/error handling needs improvement"
                exit 1
              fi
            fi
            
            echo "=== Test completed for ${{ matrix.description }} ==="

      - name: Stop qemu
        if: always()
        run: |
          sudo pkill -f qemu-system-x86_64

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: kernel-test-${{ matrix.kernel }}-logs
          path: |
            /tmp/collector-output.log
            /tmp/test-output/
          if-no-files-found: ignore
          retention-days: 5

  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
      region: ${{ steps.start-runner.outputs.region }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          iam-role-name: github-actions-runner
          instance-type: ${{ inputs.instance-type || 'c7i.metal-24xl' }}
          image-type: 'ubuntu-24.04'
          volume-size: '40'

  cancel-on-failure:
    needs: setup-runner
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Cancel workflow
        uses: andymckay/cancel-action@a955d435292c0d409d104b57d8e78435a93a6ef1

  test-ebpf:
    needs: [build-collector, setup-runner, prepare-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    steps:
      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: collector-binary
          path: ./

      - name: Make collector executable
        run: chmod +x ./collector

      - name: Run eBPF collector
        run: |
          # Run with sudo since eBPF programs require elevated privileges
          sudo ./collector -d 10 --storage-type local --prefix "/tmp/metrics-"

      - name: Verify parquet output
        run: |
          # Get the parquet file name based on the prefix /tmp/metrics
          parquet_file=$(find /tmp -name "metrics-*.parquet")

          # Print parquet file contents as CSV
          echo "Parquet file contents:"
          pqrs cat --csv $parquet_file

      - name: Upload parquet file
        uses: actions/upload-artifact@v4
        with:
          name: metrics-parquet
          path: /tmp/metrics-*.parquet
          if-no-files-found: error

  prepare-runner:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 5
    steps:
      - name: Install awscli
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          python3 -m zipfile -e awscliv2.zip .
          chmod u+x ./aws/install
          sudo ./aws/install
          echo ls: `ls -l /usr/local/bin/aws` || true
          chmod +x /usr/local/bin/aws || true
          echo version: `/usr/local/bin/aws --version` || true

      - name: Install pqrs
        run: |
          curl -L -o pqrs.zip https://github.com/manojkarthick/pqrs/releases/download/v0.3.2/pqrs-0.3.2-x86_64-unknown-linux-gnu.zip
          python3 -m zipfile -e pqrs.zip .
          sudo mv pqrs-0.3.2-x86_64-unknown-linux-gnu/bin/pqrs /usr/local/bin/
          sudo chmod +x /usr/local/bin/pqrs
          rm -rf pqrs.zip pqrs-0.3.2-x86_64-unknown-linux-gnu

      - name: Install Podman and Docker compatibility
        run: |
          # Update package list
          sudo apt-get update
          
          # Install Podman
          sudo apt-get install -y podman podman-docker
                    
          # Verify installation
          podman --version
          docker --version
          
          # Start podman socket for Docker API compatibility
          sudo systemctl enable --now podman.socket
          sudo systemctl status podman.socket
      

  test-s3-integration:
    needs: [setup-runner, prepare-runner, build-collector]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      IRSA_BUCKET: "unvariance-collector-test-irsa"
      KEY_AUTH_BUCKET: "unvariance-collector-test-key-auth"
      AWSCLI: "/usr/local/bin/aws"
    steps:


      - name: Download collector binary
        uses: actions/download-artifact@v4
        with:
          name: collector-binary
          path: ./

      - name: Make collector executable
        run: chmod +x ./collector

      # Test IAM role-based authentication (IRSA)
      - name: Test S3 with IAM Role Authentication
        id: test-iam-role
        run: |
          # Generate a unique prefix for this test
          IRSA_PREFIX=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using IRSA prefix: $IRSA_PREFIX"
          echo "irsa_prefix=$IRSA_PREFIX" >> $GITHUB_OUTPUT
          
          # Run collector with S3 output using IAM role
          echo "Running collector with IAM role authentication..."
          sudo -E AWS_BUCKET_NAME=${IRSA_BUCKET} RUST_LOG=debug ./collector -d 10 --storage-type s3 --prefix "${IRSA_PREFIX}/"
          
          # Verify the upload succeeded
          echo "Verifying S3 upload with IAM role..."
          $AWSCLI s3 ls "s3://${IRSA_BUCKET}/${IRSA_PREFIX}/"
          
          # Get uploaded file(s)
          IRSA_FILES=$($AWSCLI s3 ls "s3://${IRSA_BUCKET}/${IRSA_PREFIX}/" --recursive | awk '{print $4}')
          if [ -z "$IRSA_FILES" ]; then
            echo "No files found in IRSA bucket with prefix ${IRSA_PREFIX}"
            exit 1
          fi
          
          # Download and validate first file
          FIRST_FILE=$(echo "$IRSA_FILES" | head -n 1)
          echo "Downloading and validating file: ${FIRST_FILE}"
          $AWSCLI s3 cp "s3://${IRSA_BUCKET}/${FIRST_FILE}" /tmp/irsa-test.parquet
          
          # Validate parquet file
          echo "Validating parquet file structure:"
          pqrs cat --csv /tmp/irsa-test.parquet
          echo "IRSA test successful"

      # Test Access Key-based authentication
      - name: Test S3 with Access Key Authentication
        id: test-access-key
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
        run: |
          # Generate a unique prefix for this test
          KEY_PREFIX=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using Key Auth prefix: $KEY_PREFIX"
          echo "key_prefix=$KEY_PREFIX" >> $GITHUB_OUTPUT
          
          # Run collector with S3 output using access keys
          echo "Running collector with Access Key authentication..."
          sudo -E AWS_BUCKET_NAME=${KEY_AUTH_BUCKET} RUST_LOG=debug ./collector -d 10 --storage-type s3 --prefix "${KEY_PREFIX}/"
          
          # Verify the upload succeeded
          echo "Verifying S3 upload with Access Key..."
          $AWSCLI s3 ls "s3://${KEY_AUTH_BUCKET}/${KEY_PREFIX}/"
          
          # Get uploaded file(s)
          KEY_FILES=$($AWSCLI s3 ls "s3://${KEY_AUTH_BUCKET}/${KEY_PREFIX}/" --recursive | awk '{print $4}')
          if [ -z "$KEY_FILES" ]; then
            echo "No files found in Key Auth bucket with prefix ${KEY_PREFIX}"
            exit 1
          fi
          
          # Download and validate first file
          FIRST_FILE=$(echo "$KEY_FILES" | head -n 1)
          echo "Downloading and validating file: ${FIRST_FILE}"
          $AWSCLI s3 cp "s3://${KEY_AUTH_BUCKET}/${FIRST_FILE}" /tmp/key-auth-test.parquet
          
          # Validate parquet file
          echo "Validating parquet file structure:"
          pqrs cat --csv /tmp/key-auth-test.parquet
          echo "Access Key test successful"

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: s3-test-parquet-files
          path: |
            /tmp/irsa-test.parquet
            /tmp/key-auth-test.parquet
          if-no-files-found: error

  cleanup-runner:
    name: Stop EC2 runner
    needs: [setup-runner, test-ebpf, test-s3-integration, test-multi-kernel]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.setup-runner.outputs.region || secrets.AWS_REGION }} 