name: demo
on: 
  workflow_dispatch:  # Manual trigger for testing
    inputs:
      instance-type:
        description: 'EC2 instance type to use'
        required: false
        default: 'c5.9xlarge,c6i.16xlarge,m7i.12xlarge,c7i.12xlarge'
        type: string
      pidstat-period:
        description: 'Collection frequency for pidstat in seconds'
        required: false
        default: '1'
        type: string
  push:
    branches:
      - main
    paths:
      - deploy/opentelemetry-demo/values.yaml
      - .github/workflows/demo.yaml

permissions:
  id-token: write # Required for requesting the JWT
  contents: read
  actions: write

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          subnet-id: ${{ secrets.AWS_SUBNET_ID }}
          security-group-id: ${{ secrets.AWS_SECURITY_GROUP_ID }}
          iam-role-name: github-actions-runner          
          instance-type: ${{ inputs.instance-type || 'c5.9xlarge,c7i.12xlarge,m7i.12xlarge,c6i.16xlarge' }}
          aws-image-id: 'ami-0884d2865dbe9de4b'  # Ubuntu 22.04 LTS in us-east-2
          volume-size: '40'

  run-workload:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 30
    outputs:
      uuid-prefix: ${{ steps.generate-uuid.outputs.uuid }}
    env:
      RELEASE_NAME: otel-demo
      COLLECTOR_RELEASE_NAME: collector
      S3_BUCKET: "unvariance-collector-test-key-auth"
      AWS_REGION: ${{ secrets.AWS_REGION }}
      HOME: /root
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      PIDSTAT_PERIOD: ${{ inputs.pidstat-period || '1' }}
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: (k8s) Install K3s Cluster
        run: |
          # Installs K3s (a lightweight Kubernetes distribution) on the system
          curl -sfL https://get.k3s.io | sh

      - name: (k8s) Status of K3s Installation
        run: |
          systemctl status k3s  
      
      - name: (k8s) Wait for Kubernetes API
        run: |
          echo "Waiting for Kubernetes API..."
          until kubectl get nodes &>/dev/null; do
            sleep 1
            echo "Still waiting..."
          done
          echo "Kubernetes API is available!"

      - name: (k8s) Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: (k8s) Add Helm Repositories
        run: |
          helm repo add unvariance https://unvariance.github.io/collector/charts
          helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
          helm repo update

      - name: (k8s) Wait for nodes
        run: |
          echo "Waiting for at least one node to be registered..."
          until [ $(kubectl get nodes --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for node registration..."
          done
          echo "Node(s) registered, waiting for Ready status..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s      

      - name: (k8s) Wait for kube-system pods
        run: |
          echo "Waiting for at least one kube-system pod to be registered..."
          until [ $(kubectl get pods --namespace kube-system --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for kube-system pod registration..."
          done
          echo "Kube-system pod(s) registered, waiting for Ready status..."
          kubectl wait --namespace kube-system --for=condition=Ready pods --all --timeout=300s

      #### Install OpenTelemetry Demo
      - name: (workload) Install OpenTelemetry Demo
        run: |
          helm install $RELEASE_NAME open-telemetry/opentelemetry-demo -f deploy/opentelemetry-demo/values.yaml

      #### Install dependencies

      - name: (install) Disable IPv6
        run: |
          # Disable IPv6 via sysctl
          sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1

          # Force apt to use IPv4
          echo 'Acquire::ForceIPv4 "true";' | sudo tee /etc/apt/apt.conf.d/99force-ipv4

      - name: (install) Configure apt to use HTTPS
        run: |
          # Update all archive URLs to use HTTPS
          sudo sed -i 's/http:/https:/g' /etc/apt/sources.list

          # Install apt-transport-https (might fail initially, hence the || true)
          sudo apt-get update || true
          sudo apt-get install -y apt-transport-https ca-certificates

          # Update again with HTTPS now configure
          sudo apt-get update

      - name: (install) Install sysstat
        run: |
          sudo apt-get update
          sudo apt-get install -y sysstat linux-tools-common linux-tools-generic linux-tools-`uname -r` bzip2
          
          # Verify installations
          perf_version=$(perf --version 2>&1)
          echo "Installed perf: $perf_version"

      - name: (install) Install awscli
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          python3 -m zipfile -e awscliv2.zip .
          chmod u+x ./aws/install
          sudo ./aws/install
          echo ls: `ls -l /usr/local/bin/aws` || true
          chmod +x /usr/local/bin/aws || true
          echo version: `/usr/local/bin/aws --version` || true

      #### Wait for services to be ready


      - name: (stabilize) Wait for all Pods to be Ready
        run: |
          if ! kubectl wait --for=condition=Ready pods --all --timeout=150s; then
            echo "Error: Not all pods reached Ready state within timeout"
            kubectl get pods
            echo "--------------------------------"
            kubectl describe pods
            exit 1
          fi

      #### Show system status
      - name: (status) Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

      - name: (status) Print Events
        run: | 
          kubectl get events

      - name: (status) Disk Space Size
        run: | 
          df -h

      - name: (status) Print Pod Status
        run: | 
          kubectl get pods -n default
      
      - name: (status) Describe Frontend Deployment
        run: |
          kubectl describe deployment frontend


      #### Deploy Collector 

      - name: (collector) Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: (collector) Deploy Collector Helm Chart
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # Install the helm chart using --set for values
          helm install ${COLLECTOR_RELEASE_NAME} unvariance/collector \
            --set collector.verbose=true \
            --set storage.type=s3 \
            --set storage.prefix="${UUID_PREFIX}" \
            --set storage.s3.bucket="${S3_BUCKET}" \
            --set storage.s3.region="${AWS_REGION}" \
            --set storage.s3.auth.method=secret \
            --set storage.s3.auth.accessKey="${AWS_ACCESS_KEY_ID}" \
            --set storage.s3.auth.secretKey="${AWS_SECRET_ACCESS_KEY}" \
            --set resources.limits.cpu=1000m \
            --set resources.requests.cpu=1000m \
            --wait

      - name: (collector) Wait for Collector Pods to be Ready
        run: |
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector || WAIT_STATUS=$?
          echo "Wait exit status: $WAIT_STATUS"
          
          # Always describe pods, regardless of wait result
          echo "Describing collector pods:"
          kubectl describe pods -l app.kubernetes.io/name=collector
          
          # Only fail the job if wait failed
          if [ -n "$WAIT_STATUS" ] && [ "$WAIT_STATUS" -ne 0 ]; then
            echo "ERROR: Collector pods are not ready after timeout"
            exit 1
          fi

      - name: (collector) Show Collector Pod Status
        run: |
          kubectl get pods -l app.kubernetes.io/name=collector
          kubectl logs -l app.kubernetes.io/name=collector -c collector --tail=10

      #### Start load generator

      - name: (workload) Start Load Generator
        run: |
          helm upgrade $RELEASE_NAME open-telemetry/opentelemetry-demo -f deploy/opentelemetry-demo/values.yaml \
            --set components.load-generator.enabled=true --wait

      - name: (workload) Describe Load Generator Deployment
        run: |
          kubectl describe deployment load-generator


      #### Start PIDs stat collection

      - name: (metrics) Start PIDs stat collection
        id: start-pidstat
        run: |
          # Create directory for metrics
          mkdir -p /tmp/system_metrics
          
          # Set file paths
          CPU_METRICS_FILE="/tmp/system_metrics/cpu_metrics.csv"
          MEMORY_METRICS_FILE="/tmp/system_metrics/memory_metrics.csv"
          
          # Start CPU metrics collection in background
          pidstat -H -u -p ALL ${PIDSTAT_PERIOD} | awk '{gsub(/^ +| +$/,""); gsub(/ +/,";"); print}' > ${CPU_METRICS_FILE} 2>&1 &
          CPU_PIDSTAT_PID=$!
          
          # Start memory metrics collection in background
          pidstat -H -r -p ALL ${PIDSTAT_PERIOD} | awk '{gsub(/^ +| +$/,""); gsub(/ +/,";"); print}' > ${MEMORY_METRICS_FILE} 2>&1 &
          MEMORY_PIDSTAT_PID=$!
          
          # Verify processes are running
          ps -p $CPU_PIDSTAT_PID
          ps -p $MEMORY_PIDSTAT_PID
          
          # Set environment variables for later use
          echo "CPU_METRICS_FILE=${CPU_METRICS_FILE}" >> $GITHUB_ENV
          echo "MEMORY_METRICS_FILE=${MEMORY_METRICS_FILE}" >> $GITHUB_ENV
          echo "CPU_PIDSTAT_PID=${CPU_PIDSTAT_PID}" >> $GITHUB_ENV
          echo "MEMORY_PIDSTAT_PID=${MEMORY_PIDSTAT_PID}" >> $GITHUB_ENV
          
          echo "Started PIDs stat collection:"
          echo "CPU metrics PID: ${CPU_PIDSTAT_PID}"
          echo "Memory metrics PID: ${MEMORY_PIDSTAT_PID}"
          echo "CPU metrics file: ${CPU_METRICS_FILE}"
          echo "Memory metrics file: ${MEMORY_METRICS_FILE}"

      - name: (metrics) Record system performance with perf
        run: |
          # Create directory for perf data
          mkdir -p /tmp/perf_data
          
          echo "Starting perf record for 300 seconds at 19Hz frequency..."
          perf record -a -g -F 19 --call-graph dwarf -o /tmp/perf_data/perf.data sleep 300
          
          # Create directory for perf results
          mkdir -p perf_results
          
          # List the perf files
          echo "Perf data files:"
          ls -la /tmp/perf_data/

      - name: (metrics) Stop PIDs stat collection
        run: |
          echo "Stopping PIDs stat collection processes"
          kill -TERM $CPU_PIDSTAT_PID || true
          kill -TERM $MEMORY_PIDSTAT_PID || true
          
          # Wait a moment to ensure files are fully written
          sleep 2
          
          # Check if files exist and have content
          echo "CPU metrics file size: $(stat -c %s $CPU_METRICS_FILE || echo 'file not found')"
          echo "Memory metrics file size: $(stat -c %s $MEMORY_METRICS_FILE || echo 'file not found')"
          
          # Create directory for system metrics
          mkdir -p system_metrics
          
          # Copy the files to the upload directory
          cp $CPU_METRICS_FILE system_metrics/cpu_metrics.csv || echo "Failed to copy CPU metrics file"
          cp $MEMORY_METRICS_FILE system_metrics/memory_metrics.csv || echo "Failed to copy memory metrics file"
          
          # List the files
          echo "System metrics files:"
          ls -la system_metrics/

      - name: (metrics) Extract Locust CSV files
        run: |
          # Create directory for the results
          mkdir -p locust_results
          
          # Get the load-generator pod name
          LOADGEN_POD=$(kubectl get pods -l app.kubernetes.io/component=load-generator -o name | cut -d/ -f2)
          echo "Load Generator Pod: $LOADGEN_POD"

          # Copy CSV files from the pod
          echo "Copying CSV files from pod..."
          kubectl cp $LOADGEN_POD:/tmp/locust_results/ ./locust_results/
          
          # List the extracted files
          echo "Extracted CSV files:"
          ls -la locust_results/
          
      - name: (metrics) Upload Locust Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: locust-csv-results
          path: locust_results/
          retention-days: 28

      - name: (metrics) Upload System Metrics as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: system-metrics-results
          path: system_metrics/
          retention-days: 28

      - name: (collector) Uninstall Collector Helm Chart
        run: |
          # see the logs while uninstalling
          kubectl logs -f --tail=-1 -l app.kubernetes.io/name=collector || true &
          LOGS_PID=$!

          # wait for the pods to be deleted, in the background
          kubectl wait --for=delete pods -l app.kubernetes.io/name=collector --timeout=45s || true &
          WAIT_PID=$!

          # uninstall the chart
          helm uninstall ${COLLECTOR_RELEASE_NAME}
          
          # wait for the pods to be deleted
          wait $WAIT_PID || true

          # kill the logs
          kill -TERM $LOGS_PID || true

      - name: (collector) Get Collector Parquet File
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
        run: |
          # Get UUID prefix from the output
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # List files with the UUID prefix
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            
            # Get the file path
            PARQUET_FILE=$(echo "$S3_FILES" | head -n 1 | awk '{print $4}')
            
            # Download the parquet file for validation
            aws s3 cp "s3://${S3_BUCKET}/${PARQUET_FILE}" /tmp/collector-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/collector-parquet.parquet)
            echo "Downloaded collector file size: ${FILE_SIZE} bytes"
          fi

      - name: (collector) Upload Collector Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: collector-parquet-results
          path: /tmp/collector-parquet.parquet
          retention-days: 28

  generate-flamegraphs:
    name: Generate Flamegraphs
    needs: [setup-runner, run-workload]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    env:
      HOME: /root
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: (install) Install Rust and addr2line
        run: |
          # Install C compiler and build tools first
          sudo apt-get install -y build-essential

          # Install Rust
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
          source "$HOME/.cargo/env"
          
          # Verify Rust installation
          rustc --version
          cargo --version
          
          # Install addr2line from crates.io
          cargo install addr2line --features="bin"
          
          # Check if the system addr2line exists and back it up if it does
          if [ -f /usr/bin/addr2line ]; then
            sudo mv /usr/bin/addr2line /usr/bin/addr2line.bak
            echo "Backed up original addr2line to /usr/bin/addr2line.bak"
          fi
          
          # Copy the Rust addr2line to replace the system version
          sudo cp "$HOME/.cargo/bin/addr2line" /usr/bin/addr2line
          sudo chmod +x /usr/bin/addr2line
          
          # Verify the replacement
          addr2line --version
          echo "Replaced system addr2line with Rust implementation"
          
          # Install Inferno for flamegraph generation
          cargo install inferno
          
          # Verify Inferno installation
          inferno-flamegraph --help

      - name: (metrics) Generate Perf Report and Flamegraphs
        run: |
          # so we can use the inferno binaries
          source "$HOME/.cargo/env"

          mkdir -p flamegraph_results perf_results
          
          # Generate a report summary in text format
          echo "Generating perf report - this may take some time..."
          perf report --stdio -i /tmp/perf_data/perf.data > perf_results/perf_report.txt || true
          
          # Generate collapsed stack traces (folded)
          echo "Generating folded stacks from perf data..."
          perf script -i /tmp/perf_data/perf.data | inferno-collapse-perf > flamegraph_results/stacks.folded
          
          # Generate flamegraphs with different color schemes
          echo "Generating flamegraphs from folded stacks..."
          cat flamegraph_results/stacks.folded | inferno-flamegraph > flamegraph_results/flamegraph.svg
          
          # List the perf and flamegraph files
          echo "Perf results:"
          ls -la perf_results/
          echo "Flamegraph results:"
          ls -la flamegraph_results/

      - name: (metrics) Upload Perf and Flamegraph Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: |
            perf_results/
            flamegraph_results/
          retention-days: 28

  generate-visualizations:
    name: Generate Performance Visualizations
    needs: [run-workload]
    runs-on: ubuntu-latest
    container:
      image: rocker/tidyverse:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install R Dependencies
        run: |
          # Install only the additional packages needed (tidyverse is already included in base image)
          R -e "install.packages(c('scales', 'nanoparquet'), repos='https://cloud.r-project.org/')"

      - name: Download System Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          name: system-metrics-results
          path: system_metrics

      - name: Download Locust Results
        uses: actions/download-artifact@v4
        with:
          name: locust-csv-results
          path: locust_results

      - name: Download Collector Parquet
        uses: actions/download-artifact@v4
        with:
          name: collector-parquet-results
          path: collector_data

      - name: List Downloaded Files
        run: |
          echo "System Metrics Files:"
          ls -la system_metrics/
          
          echo "Locust Results:"
          ls -la locust_results/
          
          echo "Collector Data:"
          ls -la collector_data/

      - name: Convert CPU Metrics Format
        run: |
          # Create directory for converted metrics
          mkdir -p converted_metrics
          
          # Convert CPU metrics
          bash scripts/convert_cpu_metrics.sh system_metrics/cpu_metrics.csv converted_metrics/cpu_metrics.csv
          
          # Check output
          echo "Converted CPU Metrics:"
          head -n 5 converted_metrics/cpu_metrics.csv

      - name: Generate Memory Utilization Plots
        run: |
          mkdir -p visualization_results
          
          # Generate memory utilization plots for collector process
          Rscript scripts/plot_memory_utilization.R system_metrics/memory_metrics.csv collector visualization_results/memory_utilization
          
          # Check output
          ls -la visualization_results/

      - name: Generate CPU Utilization Plots
        run: |
          # Generate CPU utilization plots
          Rscript scripts/plot_cpu_utilization.R converted_metrics/cpu_metrics.csv collector visualization_results/cpu_utilization
          
          # Check output
          ls -la visualization_results/

      - name: Generate LLC Misses Plots
        run: |
          # Generate LLC misses plots
          Rscript scripts/plot_llc_misses.R collector_data/collector-parquet.parquet 180 0.3 visualization_results/llc_misses_0.3sec
          Rscript scripts/plot_llc_misses.R collector_data/collector-parquet.parquet 180 0.5 visualization_results/llc_misses_0.5sec
          Rscript scripts/plot_llc_misses.R collector_data/collector-parquet.parquet 180 1 visualization_results/llc_misses_1sec
          Rscript scripts/plot_llc_misses.R collector_data/collector-parquet.parquet 182 3 visualization_results/llc_misses_3sec
          Rscript scripts/plot_llc_misses.R collector_data/collector-parquet.parquet 190 10 visualization_results/llc_misses_10sec
          
          # Check output
          ls -la visualization_results/

      - name: Generate Workload Performance Plots
        run: |
          # Find the stats_history file
          STATS_FILE=$(find locust_results -name "*stats_history.csv" | head -1)
          
          if [ -n "$STATS_FILE" ]; then
            echo "Using Locust stats file: $STATS_FILE"
            # Generate workload performance plots
            Rscript scripts/plot_workload_performance.R $STATS_FILE visualization_results/workload_performance
          else
            echo "No stats_history.csv file found in locust_results directory"
            ls -la locust_results/
            exit 1
          fi
          
          # Check output
          ls -la visualization_results/

      - name: Upload Visualization Results
        uses: actions/upload-artifact@v4
        with:
          name: performance-visualizations
          path: visualization_results/
          retention-days: 28

  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, run-workload, generate-flamegraphs] # Now depends on the flamegraph job
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }} 