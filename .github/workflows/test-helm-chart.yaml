name: test-helm-chart
on: 
  workflow_dispatch:  # Manual trigger for testing
    inputs:
      instance-type:
        description: 'EC2 instance type to use'
        required: false
        default: 'm7i.xlarge'
        type: string
      image-repository:
        description: 'Custom image repository (e.g., ghcr.io/username/repo/collector)'
        required: false
        type: string
      image-tag:
        description: 'Custom image tag'
        required: false
        default: 'latest'
        type: string
  push:
    branches:
      - main
    paths:
      - 'charts/collector/**'
      - '.github/workflows/test-helm-chart.yaml'

permissions:
  id-token: write # Required for requesting the JWT
  contents: read
  actions: write

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
      region: ${{ steps.start-runner.outputs.region }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          iam-role-name: github-actions-runner
          instance-type: ${{ inputs.instance-type || 'm7i.xlarge' }}
          image-type: 'ubuntu-24.04'
          volume-size: '40'

  k3s-deployment:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    env:
      HOME: /root
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Install K3s Cluster
        run: |
          # Installs K3s (a lightweight Kubernetes distribution) on the system
          curl -sfL https://get.k3s.io | sh

      - name: Status of K3s Installation
        run: |
          systemctl status k3s  
      
      - name: Wait for Kubernetes API
        run: |
          echo "Waiting for Kubernetes API..."
          until kubectl get nodes &>/dev/null; do
            sleep 1
            echo "Still waiting..."
          done
          echo "Kubernetes API is available!"

      - name: Wait for nodes
        run: |
          echo "Waiting for at least one node to be registered..."
          until [ $(kubectl get nodes --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for node registration..."
          done
          echo "Node(s) registered, waiting for Ready status..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s      

      - name: Wait for kube-system pods
        run: |
          echo "Waiting for at least one kube-system pod to be registered..."
          until [ $(kubectl get pods --namespace kube-system --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for kube-system pod registration..."
          done
          echo "Kube-system pod(s) registered, waiting for Ready status..."
          kubectl wait --namespace kube-system --for=condition=Ready pods --all --timeout=300s

      - name: Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

  install-prerequisites:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 5
    env:
      HOME: /root
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Install awscli
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          python3 -m zipfile -e awscliv2.zip .
          chmod u+x ./aws/install
          sudo ./aws/install
          echo ls: `ls -l /usr/local/bin/aws` || true
          chmod +x /usr/local/bin/aws || true
          echo version: `/usr/local/bin/aws --version` || true

  helm-chart-deployment:
    needs: [setup-runner, k3s-deployment, install-prerequisites]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    strategy:
      matrix:
        trace-mode: [false, true]
    env:
      RELEASE_NAME: collector-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
      S3_BUCKET: "unvariance-collector-test-irsa"  # Same bucket used in IAM role testing
      AWS_REGION: ${{ secrets.AWS_REGION }}
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      HOME: /root
      IMAGE_REPOSITORY: ${{ inputs.image-repository || 'ghcr.io/unvariance/collector/collector' }}
      IMAGE_TAG: ${{ inputs.image-tag || 'latest' }}
      TRACE_MODE: ${{ matrix.trace-mode }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: Deploy Collector Helm Chart
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # Create values override file
          cat > values-override.yaml << EOF
          image:
            repository: "${IMAGE_REPOSITORY}"
            tag: "${IMAGE_TAG}"
            
          collector:
            verbose: true
            trace: ${TRACE_MODE}
          
          storage:
            type: "s3"
            prefix: "${UUID_PREFIX}"
            s3:
              bucket: "${S3_BUCKET}"
              region: "${AWS_REGION}"
              auth:
                method: "iam"  # Using IAM role
          EOF
          
          # Print the values being used
          echo "Using image: ${IMAGE_REPOSITORY}:${IMAGE_TAG}"
          
          # Install the helm chart
          helm upgrade --install ${RELEASE_NAME} ./charts/collector -f values-override.yaml

      - name: Wait for Collector Pods to be Ready
        run: |
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector
          if [ $? -ne 0 ]; then
            echo "Collector pods are not ready after timeout"
            kubectl describe pods -l app.kubernetes.io/name=collector
            exit 1
          fi

      - name: Show Pod Status
        run: |
          kubectl get pods
          kubectl describe pods -l app.kubernetes.io/name=collector

      - name: Display logs while collector runs for a while
        run: |
          timeout 10s kubectl logs -f -l app.kubernetes.io/name=collector || true
      
      - name: Uninstall Collector Helm Chart
        run: |
          helm uninstall ${RELEASE_NAME} --wait --timeout=60s

      - name: Collector logs
        run: |
          kubectl logs -l app.kubernetes.io/name=collector || true
          
      - name: Check for Files in S3
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}"
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          
          # List files with the UUID prefix
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            
            # Get the first file path
            FIRST_FILE=$(echo "$S3_FILES" | head -n 1 | awk '{print $4}')
            
            # Download the file for validation
            aws s3 cp "s3://${S3_BUCKET}/${FIRST_FILE}" /tmp/test-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/test-parquet.parquet)
            echo "Downloaded file size: ${FILE_SIZE} bytes"
            
            # We could add parquet validation here if a parquet tool is available
            echo "Helm chart S3 integration test successful"
          fi
      
      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
          path: /tmp/test-parquet.parquet
          if-no-files-found: warn

  verify-artifacts:
    name: Verify Parquet Artifacts
    needs: [helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if helm-chart-deployment fails
    strategy:
      matrix:
        trace-mode: [false, true]
    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: helm-chart-test-results-${{ matrix.trace-mode == true && 'trace' || 'aggregated' }}
          path: parquet-data

      - name: Install pqrs
        run: |
          curl -L -o pqrs.zip https://github.com/manojkarthick/pqrs/releases/download/v0.3.2/pqrs-0.3.2-x86_64-unknown-linux-gnu.zip
          python3 -m zipfile -e pqrs.zip .
          sudo mv pqrs-0.3.2-x86_64-unknown-linux-gnu/bin/pqrs /usr/local/bin/
          sudo chmod +x /usr/local/bin/pqrs
          rm -rf pqrs.zip pqrs-0.3.2-x86_64-unknown-linux-gnu
          pqrs --version

      - name: Verify Parquet File Schema and Contents
        run: |
          echo "Verifying Parquet files for ${{ matrix.trace-mode == true && 'trace' || 'aggregated' }} mode..."
          
          # Find the parquet file
          PARQUET_FILE=$(find parquet-data -name "*.parquet" -type f | head -n 1)
          
          if [ -z "$PARQUET_FILE" ]; then
            echo "ERROR: No parquet file found in artifacts"
            exit 1
          fi
          
          echo "Found parquet file: $PARQUET_FILE"
          
          # Check file size
          FILE_SIZE=$(stat -c %s "$PARQUET_FILE")
          echo "File size: ${FILE_SIZE} bytes"
          
          if [ "$FILE_SIZE" -eq 0 ]; then
            echo "ERROR: Parquet file is empty"
            exit 1
          fi
          
          # Generate and examine schema
          echo "Generating schema..."
          pqrs schema "$PARQUET_FILE" > schema.txt
          cat schema.txt
          
          # Define field lists for different modes
          if [ "${{ matrix.trace-mode }}" = "true" ]; then
            echo "Setting up trace mode field verification..."
            REQUIRED_FIELDS=("pid" "timestamp" "cpu_id" "is_context_switch" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses" "next_tgid")
          else
            echo "Setting up aggregated mode field verification..."
            REQUIRED_FIELDS=("pid" "start_time" "cgroup_id" "cache_references" "cycles" "instructions" "llc_misses")
          fi
          
          # Verify all required fields are present in schema
          echo "Verifying required fields in schema..."
          for field in "${REQUIRED_FIELDS[@]}"; do
            if ! grep -q "$field" schema.txt; then
              echo "ERROR: Required field '$field' not found in schema"
              exit 1
            else
              echo "✓ Found required field: $field"
            fi
          done
          
          # Sample records and verify content using JSON output
          echo "Sampling records with JSON output..."
          pqrs sample --records 100 --json "$PARQUET_FILE" > sample.json
          
          # Verify field values - check that each field has at least 2 different values
          echo "Verifying field values have sufficient diversity..."
          
          # Define fields that should only warn (VM might not support these performance counters)
          WARN_ONLY_FIELDS=("cache_references" "llc_misses")
          
          for field in "${REQUIRED_FIELDS[@]}"; do
            echo "Checking field: $field"
            
            # Extract all values for this field and count unique values
            UNIQUE_VALUES=$(jq -r ".${field} // null" sample.json | sort -u | wc -l)
            echo "  Unique values in $field: $UNIQUE_VALUES"
            
            if [ "$UNIQUE_VALUES" -lt 2 ]; then
              # Check if this field should only warn (performance counters that might not be supported on VM)
              if [[ " ${WARN_ONLY_FIELDS[*]} " =~ " ${field} " ]]; then
                echo "  WARNING: Field '$field' has less than 2 different values ($UNIQUE_VALUES) - this might indicate VM doesn't support this performance counter"
              else
                echo "  ERROR: Field '$field' has less than 2 different values ($UNIQUE_VALUES)"
                exit 1
              fi
            else
              echo "  ✓ Field '$field' has sufficient diversity"
            fi
          done
          
          echo "Parquet file verification completed successfully for ${{ matrix.trace-mode == true && 'trace' || 'aggregated' }} mode"

  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, k3s-deployment, install-prerequisites, helm-chart-deployment]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ needs.setup-runner.outputs.region || secrets.AWS_REGION }} 